{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68599830-49f0-4b8d-a8c3-ef61fdd161f1",
   "metadata": {},
   "source": [
    "# NeuralWave\n",
    "### [Valerio Orlandini](https://github.com/valeriorlandini/)\n",
    "[GitHub](https://github.com/valeriorlandini/neuralwavetables) | [Generator website](https://valeriorlandini.github.io/neuralwavetables/)\n",
    "\n",
    "_This code is MIT licensed_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8df7fb-9739-4f47-8841-0ec9ddeb92a2",
   "metadata": {},
   "source": [
    "If necessary, install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a452fea-4748-4183-a268-c97ad7b200ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37782fa8-97a4-41e9-8351-0de06df48d57",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522fa224-9f52-4391-baed-7a765e3d59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bef9fd-6908-4ae0-99f8-b7b577966b81",
   "metadata": {},
   "source": [
    "Set the latent space dimension, i.e. the number of parameters you will be able to tweak to generate the wavetable. In the web interface example they are 8. Experiment with other values, keeping in mind that the wavetables of the dataset are 600 samples long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442e5e5-e882-46a0-bf3b-e362b1c7b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd15335-f96e-45b3-aba6-d7188a74b3d8",
   "metadata": {},
   "source": [
    "Autoencoder implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1cbbb-3747-47e9-a7b4-07c9c7b7c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, wavetable_size, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = LATENT_DIM\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(wavetable_size, self.latent_dim * 3),  \n",
    "            nn.Tanh(),                     \n",
    "            nn.Linear(latent_dim * 3, self.latent_dim),  \n",
    "            nn.Tanh()                       \n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, wavetable_size), \n",
    "            nn.Tanh()                    \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode the input\n",
    "        encoded = self.encoder(x)\n",
    "        # Decode the encoded representation\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8d2df-ea4d-47c0-a212-a1d7db13e97e",
   "metadata": {},
   "source": [
    "Instantiate the autoencoder, using GPU if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45faa5-9883-424e-aaf7-e50e48e32b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder = Autoencoder(600, LATENT_DIM).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356aab71-d8be-43fe-a47d-a8fb31a49324",
   "metadata": {},
   "source": [
    "Load the dataset and split it between train and test, you can change the batch size if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b74945-f5e6-4182-a08a-be6fe84cfff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_all = np.load('wavetables.npy', allow_pickle = True)\n",
    "wt_train = wt_all[0:15000]\n",
    "wt_test = wt_all[15000:]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(wt_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(wt_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5e7e3-45b0-44eb-abda-310865869cde",
   "metadata": {},
   "source": [
    "Train the network, you can adjust the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57643b-17f3-4fd1-a64f-7759aef5c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    autoencoder.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    \n",
    "    autoencoder.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            outputs = autoencoder(batch)\n",
    "            loss = criterion(outputs, batch)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_dataloader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd8230-9cbf-4bb3-a3fd-a5b35193a5f2",
   "metadata": {},
   "source": [
    "Once you trained the network, you can test how it reconstructs the original wavetables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975593a-7413-44bb-b5c7-785518da2520",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "input_data = torch.tensor(wt_test[200:n+200]) \n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed = autoencoder(input_data)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.plot(input_data.numpy()[i])\n",
    "    plt.title(\"original\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(True)\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.plot(reconstructed.numpy()[i])\n",
    "    plt.title(\"reconstructed\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ec863-3ab2-42a6-b3f8-fff6ffa49eb9",
   "metadata": {},
   "source": [
    "Show the weights and the biases of the decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f0dd0-aa80-4e93-8cb5-13902e35e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = autoencoder.decoder[0]\n",
    "\n",
    "weights = linear_layer.weight\n",
    "biases = linear_layer.bias\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Biases:\", biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio3.12",
   "language": "python",
   "name": "audio3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
